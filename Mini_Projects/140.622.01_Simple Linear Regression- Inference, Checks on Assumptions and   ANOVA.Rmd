---
title: '140.622.01_Simple Linear Regression: Inference, Checks on Assumptions and
  ANOVA'
output: html_document
date: "2023-11-07"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Part.1

### 1. Inspect differences across the four groups using side-by-side box plots

**Create a new variable, agegen, to indicate the four age-gender groups. And inspect the data using side-by-side box plots: Describe the differences across the four groups in median costs, spread and shape.**

The median cost values of male aged less than 60, 4860, is similar to that of female aged less than 60, 4488. In the population aged over 60, the median costs are also similar (5904 versus 5866, respectively). We can see the group of female aged less than 60 has the largest variability across the four groups, followed by the group of female aged over 60, the group of male aged over 60, and the group of male aged less than 60. The female population has more higher outliers regardless of the age. The distributions of all groups are skewed to the right, however, it is more positively skewed in females.

```{r}
# import dataset
library(tidyverse)
ce621 = read_csv("ce621.csv")

# create new variable
ce621 = ce621 %>% 
  mutate(agegen=case_when(sex=="Male" & age<=60 ~ "m <=60",
                          sex=="Female" & age<=60 ~ "f <=60",
                          sex=="Male" & age > 60 ~ "m >60",
                          sex=="Female" & age > 60 ~ "f >60"))

# Inspect the data using side-by-side box plots:
boxplot(totchg~agegen, data=ce621, ylab="Total charges in dollars")

ce621 %>%
  group_by(agegen) %>%
  summarize(obs=n(), mean=mean(totchg), median=median(totchg), sd=sd(totchg), min=min(totchg), max=max(totchg))


```
### 2. Perform a linear regression of total charges on the age-gender groups

**Now use the data to perform a linear regression of total charges on the age-gender groups to partition the total variability as displayed in the analysis of variance (ANOVA) table for regression. Interpret each of the regression coefficients. Using regression, how do you test the overall hypothesis of no group differences? What is the difference between the results of the lm and glm commands?**

Our regression model is the following equation, and let X1 =1 if the group is f  > 60, X2 =1 if the group is m  <= 60, and X3 =1 if the group is m  > 60.

E[totchg] = β0 + β1X1 + β2X2 + β3X3 

Based on the output of the computing, we can interpret: (1) β0 = 8321.0, the expected (average) total charges for female aged less than 60. (2) β0 + β1 = -267.1, the expected (average) total charges for female aged over 60. (3) β0 + β2 = -3492.3, the expected (average) total charges for male aged less than 60. (4) β0 + β3 = -1653.0, the expected (average) total charges for male aged over 60.

We can also calculate the each value of slope: (1) β1 = - 267.1 - 8321.0 = - 8588.1, the difference in the expected (average) total charges for female aged over 60. (2) β2 = - 3492.3 - 8321.0 = - 11813.3, the difference in the expected (average) total charges for male aged less than 60. (3) β3 = - 1653.0 - 8321.0 = - 9974.0, the difference in the expected (average) total charges for male aged over 60.

We can set up Ho: β1 = β2 = β3 = 0, which is the same as Ho that there are no differences across the mean total charges of four groups. The value of the F-statistic is 1.94 on the df = 3, 196 and the p-value = 0.124 would suggest fail to rejecting Ho, which means the mean of total charges are the same across the four groups.

The lm command uses the t statistics in constructing CIs for the regression coefficients, however, the glm commands uses the z statistics in constructing CIs for the regression coefficients. 

### 3. Summarize initial findings with respect to CE costs as a function of age and gender

The median value of the carotid endarterectomy (CE) expenditure tend to increase as the age increases in both genders. However, the mean and the variance of CE cost in female are larger than that of male in both age stratum. The largely positive skewness and higher means in females show that the higher CE expenditure are needed for female population regardless of age. 

```{r}
# perform a linear regression of total charges on the age-gender groups 
# to partition the total variability as displayed in the analysis of variance (ANOVA) table for regression.
## using the lm command
model1 = lm(totchg ~ as.factor(agegen), data=ce621)
anova(model1)
summary(model1)

##using the glm command
model2 = glm(totchg ~ as.factor(agegen), data=ce621, family=gaussian(link="identity"))
anova(model2)
summary(model2)
```
## Part.2

### 4. Obtain the residuals from the regression model above and make a boxplot of the residuals by group. 

**Inspect your box plots from Step A and notice whether the observations are approximately normal and have equal variance. To remove the differences in average values among the 4 samples, we plot residuals rather than raw data by group or more often against predicted value. A residual is the difference between the observed CE cost and the predicted value from the regression. In a linear regression of continuous response on a group variable, the predicted value is just the group sample mean and the residual is just the deviation of each observation from its group mean. Obtain the residuals from the regression model above. Make a boxplot of the residuals by group. Plot the residuals against group.**

Base on the box plot from Step A, the observations are positively skewed and have different variance. The boxplots of the residuals by group are also positively skewed. The plot the residuals against the predicted values with jitter option shows that the the residuals are not Gaussian, not even approximately, and do not have equal variances. 

Given the following two assumptions of the linear regression, 
  1. Observations within a group are approximately normally distributed.
  2. The within-group variance is the same across all groups.

I need to do transformation of the value of outcome variable.

```{r}
#Inspect your box plots from Step A 
model1 = lm(totchg ~ as.factor(agegen), data=ce621)
boxplot(model1$residuals ~ ce621$agegen, ylab="Residuals")

# Plot the residuals against the predicted values.
qplot(x=model1$fitted.values, y=model1$residuals, xlab="Predicted values", ylab="Residuals")

# [+] This graph can be improved by using the jitter option.
qplot(x=jitter(model1$fitted.values), y=model1$residuals,xlab="Fitted values", ylab="Residuals")
```

### 5. Transform the CE expenditure data to address this problem.

**One way to address this problem is by analyzing a transformation of the CE expenditure data, rather than the data on its original scale. This works if you want to ask questions about whether there are differences between groups rather than estimating the size of the differences. To accomplish this, generate a new variable which is the logarithm(log10)of CE expenditures. Make the graphical display using box plots, as was done above.**

When I see the box plot with a new variable with log transformation,the distribution of the transformed CE data in Step B is more normally distributed compared to the previous distribution of the untransformed CE data in Step A. The within-group variances in Step B are more nearly equal across groups than the variances in Step A.

However, we need more statistical evidence with CIs or p-value in order to make the equal variance assumption to make the assumption of equal (constant) variance.

```{r}
# to adress this problem, -> analyzing a transformation of the CE expenditure data, rather than the data on its original scale.
# transformation 
ce621 = ce621 %>%
  mutate(logtotchg=log10(totchg))
model3 = lm(logtotchg ~ as.factor(agegen), data=ce621)
summary(model3)

# boxplot with residuals
boxplot(model3$residuals ~ ce621$agegen, ylab="Residuals")
```
## Part 3.

### 6. Use bootstrapping to get more appropriate standard errors

**Another way to proceed when the focus is the difference in the means themselves, not the means of a transformed value, is to use regression to estimate the means but to use bootstrapping to get more appropriate standard errors that do not depend on the normal and equal variance assumptions. Compare the bootstrap standard errors and confidence intervals with the ones from the original regression analysis. These are more valid when the assumptions are so strongly violated.**

The value of standard error in bootstrap, which are falled between 2327.765 to 2457.763, is smaller than that from the original regression analysis, 5272.648 (calculated by the root of MSE). The confidence intervals are also narrow in the bootstrap. We can see the bootstrap method can provide more valid and precise results under the violation of the normality assumption. 

The mean of CE cost in females (8321 for those aged less than 60, 8054 for those aged over 60) are larger than that of males (4829 for those aged less than 60, 6668 for those aged over 60). We can see the groups of female have larger variability and more upper outliers than the groups of males regardless of age. The distributions of all groups are skewed to the right, however, it is more positively skewed in females.

Based on the regression analysis, no statistically significant difference in the means of CE cost is observed across the four groups, as evidenced by p value of F test, 0.124. However, according to the bootstrap, there is significant difference in the mean of CE cost of female aged less than 60 compared with the others. In this case, where the normality assumption does not hold, the bootstrap can provide more precise estimation. As a result, females aged less than 60 has statistically significant higher mean CE cost compared with the other gender and age strata.


```{r}
# setting for bootstrapping
library(boot)

# function to obtain regression coefficients
bs = function(formula, data, indices) {
  d=data[indices, ] # allows boot to select sample 
  fit = lm(formula, data=d)
  return(coef(fit))
}

# bootstrapping with 250 replications
results = boot(data=ce621, statistic=bs, R=250, formula=totchg~agegen)
results

# get 95% confidence intervals from the bootstrap
boot.ci(results, type="norm", index=1) # intercept (f <=60)
boot.ci(results, type="norm", index=2) # f >60
boot.ci(results, type="norm", index=3) # m <=60
boot.ci(results, type="norm", index=4) # m >60

# get 95% confidence intervals from the regression model
confint(model1)
```


